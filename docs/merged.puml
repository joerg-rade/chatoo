@startuml
title Model Context Protocol (MCP) – Full System Overview

'────────────────────────────────────────────
' User Side
'────────────────────────────────────────────
package "User-Side" {
    component "<<UI Client>>\n<<Client App>>\nChatBox\n[[https://github.com/chatboxai/chatbox]]" as UI
    component "<<Library/API>>\nLLM API\nMCP Client (mcp-use)\n[[https://github.com/mcp-use]]" as LLMAPI
    UI -r-> LLMAPI
}

'────────────────────────────────────────────
' MCP Server Core
'────────────────────────────────────────────
package "MCP Server" {
    component Agent
    component "Host\n(Plugins / APIs)" as Host
}

component "[LLM]\n(Language Model)" as LLM

LLMAPI -> Agent : Prompt \n/ Request
Agent -l-> LLM : Enriched Input
Agent -> Host : JSON/RPC\ncalls
Host --> Agent : HTTP/SSE\nstream/events
Agent -l-> LLMAPI : Final Response

note top of Agent
The Agent acts as the orchestrator between components:
* Receives and preprocesses requests from Client
* Injects user/session/context metadata
* Mediates and formats input to LLM
* Manages conversation state and sessions
* Routes tool invocations via Host
* Aggregates and returns responses
* Enforces safety, policy, and access control
end note

note bottom of Host
The Host facilitates external capabilities:
* Exposes plugins and tool APIs to the Agent
* Manages execution lifecycles of tools
* Streams data/results using HTTP or SSE
* Acts as a secure boundary between Agent and tools
* Supports scaling and routing to external services
end note

'────────────────────────────────────────────
' LLM Layer
'────────────────────────────────────────────
package "LLM" {
    component OLLAMA {
        [<<LLM Engine/Provider>>\nAgent (LLM / LangChain)\nDeepSeek] as MODEL
        LLM <-> MODEL
    }
}

note top of MODEL
    * OpenAI
    * Ollama
    *<u>DeepSeek</u> (Distilled/Q4)
    chosen because it's small
end note

'────────────────────────────────────────────
' Tool Provider & OpenAPI Side
'────────────────────────────────────────────
package "Tool-Provider Side" as TPS {
    component "<<OpenAPI Adapter>>\nMCP OpenAPI Server\n[[https://github.com/ivo-toby/mcp-openapi-server]]" as OAA
}

OAA -d-> [<<Restful Objects API>>\nApache Causeway] : RESTful API

'────────────────────────────────────────────
' MCP Client Notes
'────────────────────────────────────────────
note top of LLMAPI
* <b>Dynamic Agent Orchestration:</b>
  The client app uses mcp-use to connect to <b>multiple MCP servers</b> simultaneously,
  with the ability to <b>dynamically select</b> which server to use per task.

* <b>Flexible Server Config:</b>
  MCP servers (local/remote) are <b>configurable via JSON</b> or <b>programmatically</b>.

* <b>LLM Interoperability:</b>
  Supports standard <b>LangChain adapters</b> across OpenAI, Ollama, DeepSeek.

* <b>Tool Access Control:</b>
  Agent tool access is selectively permissioned via mcp-use configuration.

* <b>Hosted & Web Integration:</b>
  Compatible with headless, hosted, or browser-based environments.
end note

Host -r-> TPS : Triggers or\nInvokes Tools
TPS -l-> Host : Tool Results

@enduml
