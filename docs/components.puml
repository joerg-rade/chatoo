@startuml
title Model Context Protocol (MCP) / OpenAPI Server Overview

component "<<UI Client>>\nOpen WebUI\n[[https://github.com/open-webui]]" as UI {
    port "3000" as UIP
}

component "<<LLM Adapter>>\nOllama " as OLLAMA {
    port "11434" as OP
    database "<<LLM Engine>>\n* DeepSeek\n* Phi3" as LLM
}

component "<<OpenAPI Adapter>>\nMCP OpenAPI Server\n[[https://github.com/ivo-toby/mcp-openapi-server]]" as OAA {
    port "8000" as OAP
}

component "<<Restful Objects API>>\nApache Causeway" as RO {
    port "8080" as ROP
}

OAA --> ROP : RESTful API
UI <--> OP
UI --> OAP

note bottom of LLM
    Small, Quantized, Distilled models
    allow use on consumer hardware
end note

/'note top of UI
* <b>Dynamic Agent Orchestration:
    The client app uses mcp-use to connect to <b>multiple MCP servers simultaneously,
    with the ability to <b>dynamically select</b> which server to use for each task.
    This enables modular, agent-based orchestration of AI workflows.
* <b>Flexible Server Config:
    MCP servers — either <b>local or remote</b> — can be <b>configured declaratively via JSON
    or <b>programmatically</b>, allowing adaptive deployments tailored to different
    runtime contexts or tools.
* <b>LLM Interoperability:
    Language models (e.g. OpenAI, Ollama, DeepSeek) are accessed via standard
    <b>LangChain adapters</b>, abstracting away vendor-specific logic and enabling seamless
    switching or combination of models.
* <b>Tool Access Control:
    Agents can be granted selective access to tools (e.g. files, APIs, memory) via centralized
    configuration in mcp-use, ensuring intentional permissioning and reduced risk of unintended
    tool invocation.
* <b>Hosted & Web Integration:
    mcp-use is designed to operate in <b>headless, hosted, or browser-based environments,
    supporting both <b>local dev</b> and <b>production deployments.
end note

@enduml
